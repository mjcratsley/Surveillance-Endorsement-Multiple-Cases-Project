---
title: "Surveillance Endorsement in Multiple Cases"
author: "Maya Cratsley"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Prospectus
Question: When and why do people endorse surveillance?

Previous academic research has examined and identified several factors driving people’s willingness to be monitored, including: ignorance (not knowing the extent to which we are being watched; Hoofnagle et al. 2010), mutual benefit (getting something in return; PEW Research, 2014), perceived inevitability (Fast & Jago 2020), desire to divulge (mixed motives; John, 2016), lack of immediate negative outcomes (Fast & Jago 2020), and intangibility of privacy as a concept (John, 2016). 

However, most research thus far on people’s willingness to be monitored has not made clear distinctions between surveillance and other passive data collection, and may have been intended to apply specifically to the latter. While all data collection represents a reduction in privacy, some can be seen as serving a purely informational function, while other data collection also has a control function. We thus define surveillance as: “observation of an individual by an institution or individual who holds power over them, for the purpose of influence, management, and control.” This definition is important because it highlights a key feature of surveillance technologies: their potential to infringe on not only people's privacy, but also people's autonomy. 

The potential for surveillance to infringe on autonomy is important because, theoretically speaking,it means that people should oppose surveillance that applies to themselves. Basic Psychological Needs Theory, a component of Self-Determination Theory, posits that autonomy is one of the three fundamental psychological desires of human beings, meaning that infringements on autonomy, such as surveillance, would be avoided if at all possible (Deci & Ryan, 1985; Deci & Ryan 2000, Ryan & Deci 2000). However, in real life we have seen a steady increase in the degree to which we allow both private and public institutions to surveil us. This is the puzzle that this research adresses. We ultimately argue that people will endorse their own surveillance, and will even make sacrifices in order to be surveilled, when social contextual factors allow them to perceive the surveillance as not presenting any threat to their autonomy. 
  
Individuals typically have access to social-relational information that they use to infer the impact a given initiative will have on themselves, and whether it may result in a decrease to their autonomy.Specifically, the perceived threat to autonomy that is posed by surveillance may be impacted by individuals’ relationship to the organization or person doing the surveillance. The degree to which we see the person or group surveilling us as aligned with ourselves (perceived self-other alignment) informs our response to surveillance initiatives, by encouraging individuals to believe that the surveiller will likely take little issue with their current behaviors. This leads people to believe that they will not have to change their behaviors, and thus give up their autonomy, in order to avoid punishment. For example, if you are a member of the LGBTQ+ community, and you see your government as aligned with you, you may not be concerned about needing to avoid being seen with your partner in order to avoid governmental sanctions. Thus, governmental surveillance may be seen as not infringing upon your autonomy. This study aims to test the claim that the degree to which we see ourselves as aligned with a given institution will predict our willingness to be surveilled by that institution.

Design:

The study was hosted on Qualtrics, and distributed through the USC SONA system. Participants who consented to participate first read some information about "new technological tools" for tracking their internet activity. They were then asked to rate for 10 different groups (the police, google, the CDC, etc): the degree to which they see themselves as aligned with those groups (SOA- IV, mod. from Aron, Aron, & Smollan, 1992) and how much they would endorse the use of these tools by those groups (endorsement- DV). The order in which the SOA and endorsement measures will be presented was randomized, such that half of the participants saw the IV first and half the DV. Additionally, for each measure (SOA and Endorsement), the groups they were asked to evaluate were presented in a random order.

The data has a cross-classified structure with level 2a representing clustering by participant and level 2b representing clustering by cases. The cluster size for participant= 10, and the cluster size for cases=~200. 

Primary analyses (preregistered):

We will use the lme4 package (Bates, Maechler & Bolker, 2012) in R to fit a linear mixed model using the restricted maximum likelihood estimation method. The only fixed effect in the model will be that of SOA on Endorsement. Participant id and target case (the police, Amazon, etc.) will be included as random intercepts to account for across-participant variations in overall endorsement of surveillance, and variation in endorsement across the various cases. Prior to running hypothesis-testing analyses, we will examine the residuals plot of the data to ensure that the data does not demonstrate any obvious deviations from homoskedasticity and normality. Provided the data doesn't violate assumptions of our analyses and we can proceed as planned, we will then perform a likelihood ratio test to compare the full model with the single fixed effect and two random effects to a null model that is identical but excludes the fixed effect of SOA on Endorsement. If the likelihood ratio test yields a p value <0.05, this will be interpreted as indicating statistical significance of the overall effect of SOA on Endorsement, and support for our hypothesis.

Additional analyses (not pre-registered):

As an exploratory measure, we will estimate two additional models, one that includes a random slopes for participant id, and one that includes a random slope for cases. Each of these models will be tested to see whether the random slopes are significant. If either/both of the random slopes are significant, we will estimate a model that includes the significant slopes, and perform likelihood ratio tests comparing that model to the null model, as well as to the pre-registered hypothesized model. We will also calculate and report ICCs for both participant id and cases, to demonstrate that the variance across cases and across participants was non-negligible.


## Load Packages
```{r load, include=FALSE}
library(here)  # makes reading data more consistent
library(tidyverse)  # for data manipulation and plotting
library(haven)  # for importing SPSS/SAS/Stata data
library(lme4)  # for multilevel analysis
library(lattice)  # for dotplot (working with lme4)
library(sjPlot)  # for plotting effects
library(MuMIn)  # for computing r-squared
library(r2mlm)  # for computing r-squared
library(broom.mixed)  # for summarizing results
library(modelsummary)  # for making tables
library(lmerTest)
theme_set(theme_bw())  # Theme; just my personal preference
#install and load packages, including the lme4 package (Bates,Maechler & Bolker,2012)

```

```{r mmps_lmer}
#Loading Mark's function for marginal model plots
mmps_lmer <- function(object) {
  plot_df <- object@frame
  form <- formula(object)
  xvar <- attr(attr(plot_df, "terms"), "varnames.fixed")[-1]
  plot_df$.fitted_x <- fitted(object)
  plot_df$.fitted <- plot_df$.fitted_x
  plot_df$.rowid <- seq_len(nrow(plot_df))
  plot_df_long <- reshape(plot_df, direction = "long",
                          varying = c(xvar, ".fitted_x"),
                          v.names = "xvar",
                          idvar = ".rowid")
  plot_df_long$varname <- rep(c(xvar, ".fitted"),
                              each = nrow(plot_df))
  ggplot(
    data = plot_df_long,
    aes_string(x = "xvar", y = paste(form[[2]]))
  ) +
    geom_point(size = 0.5, alpha = 0.3) +
    geom_smooth(aes(col = "data"), se = FALSE) +
    geom_smooth(aes(y = .fitted, col = "model"),
      linetype = "dashed", se = FALSE
    ) +
    facet_wrap(~ varname, scales = "free_x") +
    labs(color = NULL, x = NULL) +
    scale_color_manual(values = c("data" = "blue",
                                  "model" = "red")) +
    theme(legend.position = "bottom")
}
```

## Load data
Convert to long format

```{r load2}

surveillancedt <- read.csv("~/Desktop/Surveillance/Endorsement/MultCase1.csv")
#load the data into a dataframe

# Convert to long format using the new `tidyr::pivot_longer()` function
surveillancedt_long <- surveillancedt %>%
  pivot_longer(
    c(SOA_1:SOA_10,Endorsement_1:Endorsement_10),  # variables that are repeated measures
    # Convert 20 columns to 3: 1 columns each for SOA/Endorsement (.value), and
    # one column for case
    names_to = c(".value", "Case"),
    # Extract the names "SOA"/"Endorsement" from the names of the variables for the
    # value columns, and then the number to the "Case" column
    names_pattern = "(SOA|Endorsement)_([1-10])",
    # Convert the "Case" column to integers
    names_transform = list(Case = as.integer)
  )
surveillancedt_long %>% 
  select(ResponseId, SOA, Endorsement, Case, everything())

#for some reason this doesn't work and erases the data for every case besides case 1
#so I will not be using this data for these initial analyses
#I have a long form version already created that I will load instead
```


```{r data, include=FALSE}

#writing over the previous data table with my long form data file

surveillancedt <- read.csv("~/Desktop/Surveillance/Endorsement/MultCase1_reformatted.csv")
#load the data into a dataframe

#dropping the participants with some nas
surveillancedt_clean <- drop_na(surveillancedt)

#uncomment the following if you want to check that the data loaded properly
#View(surveillancedt)
#head(surveillancedt)
#tail(surveillancedt)
#summary(surveillancedt)
#str(surveillancedt)
#colnames(surveillancedt)
#which(is.na(surveillancedt)==T) #checks for missing values

```

### Equations

Repeated-Measure level (Lv 1):
$$\text{Endorsement}_{i(j,k)} = \beta_{0(j, k)} + e_{ijk}$$

Between-cell (Subject $\times$ Item) level (Lv 2):
$$\beta_{0(j, k)} = \gamma_{00} + \beta_{1j} \text{Alignment}_{k}+\beta_{2k} \text{Alignment}_{j} +u_{0j} + v_{0k}$$
Subject level (Lv 2a) random slopes
$$\beta_{1j} = \gamma_{10} + u_{1j}$$

Case level (Lv 2b) random slopes
$$\beta_{2k} = \gamma_{20} + v_{2k}$$
Full Model Equation:

$$\text{Endorsement}_{i(j,k)} = \gamma_{00} + (\gamma_{10} + u_{1j}) \text{Alignment}_{k}+(\gamma_{20} + v_{2k}) \text{Alignment}_{j} +u_{0j} + v_{0k} + e_{ijk}$$
## Building the Model

I'll estimate the preregistered model (equation above).

```{r Main Model}
surveillance.model <- lmer(Endorsement.Values ~ Alignment.Values + (1 | ResponseId)+ (1 | Case), data=surveillancedt_clean)
#create and view our pre-registered mixed effects model with 1 fixed effect and 2 random intercepts

summary(surveillance.model)
#view the summary of our model to help with interpretation
#intercept estimate tells you the anticipated endorsement level at alignment=0
#alignment values ~i think~ tells you the expected change in endorsement per point of aligment change
confint(surveillance.model, parm = "beta_")
#confidence interval does not cross 0, so null is rejected ~but by how much~?

```

## Visualizing the Data

```{r boxplot}
boxplot(Endorsement.Values ~ Case,
        col=c("white","lightgray"),surveillancedt_clean)
#look at the variation in mean Endorsement between cases
```
On this chart, the labels are as follows:

1- Your place of employment

2-the police

3- apple

4-amazon

5-google

6-facebook

7-The National Science Foundation

8- The US Military

9- The CDC

10- The US government


```{r assumptions}
#testing for linearity by making a residuals plot using Mark's function
mmps_lmer(surveillance.model)

#Testing for homoscedasticity across Alignment Values (level 1)
augment(surveillance.model) %>%
  mutate(.std_resid = resid(surveillance.model, scaled = TRUE)) %>%
  ggplot(aes(x = Alignment.Values, y = .std_resid)) +
  geom_point(size = 0.7, alpha = 0.5) +
  geom_smooth(se = FALSE)

#Testing for normality at level 1
library(lattice)  # need this package to use the built-in functions
qqmath(surveillance.model)  # just use the `qqmath()` function on the fitted model

#Testing for normality at other levels
qqmath(ranef(surveillance.model, condVar = FALSE),
       panel = function(x) {
         panel.qqmath(x)
         panel.qqmathline(x)
       })

#plot(fitted(surveillance.model),residuals(surveillance.model))
#another plot I could use to check for linearity, normality & homoskedasticity 
```
The test for linearity shows some minor deviations from linearity. Homoscedasticity is demonstrated, confirming the equal variance assumption. The normality assumption is also confirmed at the data and participant level, however there is some variance across cases.

## Model Estimates

```{r calculate ICCs}
#calculate ICC for Endorsement based on our two intercepts


surveillance.null <- lmer(Endorsement.Values ~ (1 | ResponseId) + (1 | Case), data=surveillancedt_clean)
#create our null model, which excludes the fixed effect of alignment

vc_null <- as.data.frame(VarCorr(surveillance.null))
# Proportion of variance at the within-cell level
icc_e <- vc_null$vcov[3] / sum(vc_null$vcov)

# ICC/Deff (person; cluster size= 266
icc_person <- vc_null$vcov[1] / sum(vc_null$vcov)
c("ICC(person)" = icc_person,
"Deff(person)" = icc_e + 266 * icc_person)

# ICC/Deff (Case; cluster size = 10)
icc_case <- vc_null$vcov[2] / sum(vc_null$vcov)
c("ICC(case)" = icc_case,
"Deff(case)" = icc_e + 10 * icc_case)

```

The ICC for "the variance between "ResponseId"  is 0.4137, indicating that 41% of the variance in endorsement is explained by between-person differences. The ICC for "Case" is 0.0681, indicating that 6% of the variance in endorsement is explained by differences between cases. Thus, it seems that it was appropriate to include these two random intercepts into the model.

```{r summary table}
msummary(list(Estimate = surveillance.model, `95% CI` = surveillance.model),
estimate = c("estimate", "[{conf.low}, {conf.high}]"),
statistic = NULL, # suppress the extra rows for SEs
title = "Parameter Estimates of Multilevel Model",
gof_omit = ".*")

```

```{r plot}
#plotting the model predictions next to the data
sjPlot::plot_model(surveillance.model, type = "pred", terms = "Alignment.Values", 
                   show.data = TRUE, title = "", 
                   dot.size = 0.5, jitter =0.2)
```


## Alternative Models

I will also test random slopes for Case and ResponseId to see whether they are worth including in the model.

```{r Alt Models}

surveillance.model2 <- lmer(Endorsement.Values ~ Alignment.Values + (Alignment.Values | ResponseId)+ (1 | Case), data=surveillancedt_clean)
#create a model that includes random slopes for participants
ranova(surveillance.model2)
#test whether the random slope is significant

surveillance.model3 <- lmer(Endorsement.Values ~ Alignment.Values + (1 | ResponseId)+ (Alignment.Values | Case), data=surveillancedt_clean)
#create a model that includes random slopes for case
ranova(surveillance.model3)
#test whether the random slope is significant

```

Both random slopes were significant, so I will build a model with both included.

```{r Random Slope Model}

surveillance.model4 <- lmer(Endorsement.Values ~ Alignment.Values + (Alignment.Values | ResponseId)+ (Alignment.Values | Case), data=surveillancedt_clean)

summary(surveillance.model4)
#creating and viewing a model with random slopes
```


## Likelihood Ratio Test

```{r LRT}

surveillance.null

#view our null model, which excludes the fixed effect of alignment

anova(surveillance.null, surveillance.model)

#using the anova function to perform the Likelihood Ratio Test which will compare our model to the null
# The hypothesized model is a significant improvement on the null
#reporting: Alignment significantly increased endorsement (chisq("DF")="chisq", p="PR(>Chisq)"). 

anova(surveillance.model,surveillance.model4)
#the model with random slopes included for both Case and Participant is a significant improvement on the hypothesized model


```

The likelihood ration tests demonstrate that the hypothesized model is a significant improvement on the null model, but the alternative model, which includes random slopes, is a significant improvement on the hypothesized model.These results largely indicate that Alignment was a significant predictor of Endorsement, such that a 1 point increase in Alignment predicted about a 0.3 point increase in endorsement. This provides support for the theoretical perspective being tested.
